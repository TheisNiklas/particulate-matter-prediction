{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import neptune\n",
    "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from window_generator_categorical import WindowGenerator\n",
    "\n",
    "from run_automation_utils import Params, init_model\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_window_generator(station_id):\n",
    "    df = pd.read_feather(f\"../../data/pollution/processed/categorised/pm10/{station_id}.feather\")\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "    \n",
    "    timestamp_s = df.index.map(pd.Timestamp.timestamp)\n",
    "\n",
    "    day = 24 * 60 * 60\n",
    "    year = (365.2425) * day\n",
    "\n",
    "    df[\"Day sin\"] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    df[\"Day cos\"] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    df[\"Year sin\"] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    df[\"Year cos\"] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Convert to radians.\n",
    "    wd_rad = df.pop(\"winddirection_10m\") * np.pi / 180\n",
    "    df[\"winddirection_10m_sin\"] = np.sin(wd_rad)\n",
    "    df[\"winddirection_10m_cos\"] = np.cos(wd_rad)\n",
    "\n",
    "    df[\"kategorie\"] = df[\"kategorie\"].astype(np.int32)\n",
    "\n",
    "    return WindowGenerator(input_width=24 * 7, label_width=24, shift=24, df=df.copy(), label_columns=[\"kategorie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_generator_538 = prep_window_generator(538)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: is that needed? train one with and one without (cnn_lstm_model.add(tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(include_lambda: bool = False):\n",
    "    CONV_WIDTH = 3\n",
    "    OUT_STEPS = 24  # 14 days prediction\n",
    "    pred_categories = 6  # TODO: ??? num_features is 11 but must be one for our prediction\n",
    "\n",
    "    cnn_lstm_model = tf.keras.models.Sequential()\n",
    "    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\n",
    "    if include_lambda:\n",
    "        cnn_lstm_model.add(tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]))\n",
    "    # Shape => [batch, 1, conv_units]\n",
    "    cnn_lstm_model.add(tf.keras.layers.Conv1D(256, activation=\"relu\", kernel_size=(CONV_WIDTH)))\n",
    "    cnn_lstm_model.add(tf.keras.layers.MaxPooling1D())\n",
    "    cnn_lstm_model.add(tf.keras.layers.BatchNormalization())  # TODO: useful to put it here?\n",
    "    cnn_lstm_model.add(tf.keras.layers.Conv1D(512, activation=\"relu\", kernel_size=(CONV_WIDTH)))\n",
    "    cnn_lstm_model.add(tf.keras.layers.MaxPooling1D())\n",
    "    cnn_lstm_model.add(tf.keras.layers.BatchNormalization())  # TODO: useful to put it here?\n",
    "    cnn_lstm_model.add(tf.keras.layers.Conv1D(512, activation=\"relu\", kernel_size=(CONV_WIDTH)))\n",
    "\n",
    "    #cnn_lstm_model.add(tf.keras.layers.Flatten())\n",
    "    # LSTM\n",
    "    cnn_lstm_model.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "    cnn_lstm_model.add(tf.keras.layers.LSTM(32, return_sequences=False))\n",
    "\n",
    "    cnn_lstm_model.add(tf.keras.layers.Dense(512))\n",
    "    cnn_lstm_model.add(tf.keras.layers.Dropout(0.4))\n",
    "    # Shape => [batch, out_steps, features] -> for each prediction step one neuron\n",
    "    # Anpassung fÃ¼r die Klassifizierung\n",
    "    cnn_lstm_model.add(tf.keras.layers.Dense(OUT_STEPS * pred_categories, activation=\"softmax\"))\n",
    "\n",
    "    # Shape => [batch, out_steps, pred_categories]\n",
    "    cnn_lstm_model.add(tf.keras.layers.Reshape([OUT_STEPS, pred_categories]))\n",
    "    return cnn_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneWarning: To avoid unintended consumption of logging hours during interactive sessions, the following monitoring options are disabled unless set to 'True' when initializing the run: 'capture_stdout', 'capture_stderr', and 'capture_hardware_metrics'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/data-mining-team2/model-tests/e/MOD-6\n",
      "(None, 24, 1)\n",
      "(None, 24, 1)\n",
      "Epoch 1/50\n",
      "682/682 [==============================] - 84s 117ms/step - loss: 0.7486 - accuracy: 0.6625 - val_loss: 0.7274 - val_accuracy: 0.6771\n",
      "Epoch 2/50\n",
      "682/682 [==============================] - 82s 120ms/step - loss: 0.6993 - accuracy: 0.6815 - val_loss: 0.6961 - val_accuracy: 0.6841\n",
      "Epoch 3/50\n",
      "682/682 [==============================] - 86s 126ms/step - loss: 0.6817 - accuracy: 0.6919 - val_loss: 0.6752 - val_accuracy: 0.6935\n",
      "Epoch 4/50\n",
      "682/682 [==============================] - 85s 124ms/step - loss: 0.6685 - accuracy: 0.6973 - val_loss: 0.6681 - val_accuracy: 0.6996\n",
      "Epoch 5/50\n",
      "682/682 [==============================] - 86s 127ms/step - loss: 0.6532 - accuracy: 0.7064 - val_loss: 0.6814 - val_accuracy: 0.6917\n",
      "Epoch 6/50\n",
      "682/682 [==============================] - 88s 129ms/step - loss: 0.6428 - accuracy: 0.7125 - val_loss: 0.7036 - val_accuracy: 0.6876\n",
      "Epoch 7/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.6376 - accuracy: 0.7162 - val_loss: 0.6765 - val_accuracy: 0.6917\n",
      "Epoch 8/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.6234 - accuracy: 0.7218 - val_loss: 0.6228 - val_accuracy: 0.7214\n",
      "Epoch 9/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.6049 - accuracy: 0.7321 - val_loss: 0.6130 - val_accuracy: 0.7354\n",
      "Epoch 10/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.5856 - accuracy: 0.7434 - val_loss: 0.5981 - val_accuracy: 0.7431\n",
      "Epoch 11/50\n",
      "682/682 [==============================] - 88s 129ms/step - loss: 0.5686 - accuracy: 0.7526 - val_loss: 0.5718 - val_accuracy: 0.7541\n",
      "Epoch 12/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.5495 - accuracy: 0.7621 - val_loss: 0.5402 - val_accuracy: 0.7690\n",
      "Epoch 13/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.5264 - accuracy: 0.7734 - val_loss: 0.5924 - val_accuracy: 0.7367\n",
      "Epoch 14/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.4997 - accuracy: 0.7865 - val_loss: 0.5112 - val_accuracy: 0.7829\n",
      "Epoch 15/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.4739 - accuracy: 0.7999 - val_loss: 0.4828 - val_accuracy: 0.7989\n",
      "Epoch 16/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.4498 - accuracy: 0.8116 - val_loss: 0.5280 - val_accuracy: 0.7805\n",
      "Epoch 17/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.4303 - accuracy: 0.8211 - val_loss: 0.4373 - val_accuracy: 0.8187\n",
      "Epoch 18/50\n",
      "682/682 [==============================] - 88s 129ms/step - loss: 0.4128 - accuracy: 0.8288 - val_loss: 0.4182 - val_accuracy: 0.8251\n",
      "Epoch 19/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.3973 - accuracy: 0.8352 - val_loss: 0.4734 - val_accuracy: 0.8044\n",
      "Epoch 20/50\n",
      "682/682 [==============================] - 90s 133ms/step - loss: 0.3762 - accuracy: 0.8444 - val_loss: 0.4125 - val_accuracy: 0.8283\n",
      "Epoch 21/50\n",
      "682/682 [==============================] - 92s 134ms/step - loss: 0.3634 - accuracy: 0.8494 - val_loss: 0.4319 - val_accuracy: 0.8184\n",
      "Epoch 22/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.3528 - accuracy: 0.8539 - val_loss: 0.3689 - val_accuracy: 0.8490\n",
      "Epoch 23/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.3375 - accuracy: 0.8601 - val_loss: 0.3351 - val_accuracy: 0.8590\n",
      "Epoch 24/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.3271 - accuracy: 0.8644 - val_loss: 0.3554 - val_accuracy: 0.8528\n",
      "Epoch 25/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.3168 - accuracy: 0.8679 - val_loss: 0.3426 - val_accuracy: 0.8576\n",
      "Epoch 26/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.3056 - accuracy: 0.8731 - val_loss: 0.3209 - val_accuracy: 0.8679\n",
      "Epoch 27/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.2986 - accuracy: 0.8753 - val_loss: 0.3306 - val_accuracy: 0.8610\n",
      "Epoch 28/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.2907 - accuracy: 0.8787 - val_loss: 0.2988 - val_accuracy: 0.8772\n",
      "Epoch 29/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.2839 - accuracy: 0.8807 - val_loss: 0.3169 - val_accuracy: 0.8675\n",
      "Epoch 30/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.2771 - accuracy: 0.8837 - val_loss: 0.2881 - val_accuracy: 0.8805\n",
      "Epoch 31/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.2694 - accuracy: 0.8864 - val_loss: 0.2982 - val_accuracy: 0.8779\n",
      "Epoch 32/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.2635 - accuracy: 0.8884 - val_loss: 0.2619 - val_accuracy: 0.8898\n",
      "Epoch 33/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.2572 - accuracy: 0.8907 - val_loss: 0.2853 - val_accuracy: 0.8815\n",
      "Epoch 34/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.2495 - accuracy: 0.8948 - val_loss: 0.2724 - val_accuracy: 0.8860\n",
      "Epoch 35/50\n",
      "682/682 [==============================] - 95s 140ms/step - loss: 0.2445 - accuracy: 0.8964 - val_loss: 0.2543 - val_accuracy: 0.8933\n",
      "Epoch 36/50\n",
      "682/682 [==============================] - 104s 153ms/step - loss: 0.2392 - accuracy: 0.8988 - val_loss: 0.2397 - val_accuracy: 0.8975\n",
      "Epoch 37/50\n",
      "682/682 [==============================] - 102s 150ms/step - loss: 0.2337 - accuracy: 0.9010 - val_loss: 0.2744 - val_accuracy: 0.8865\n",
      "Epoch 38/50\n",
      "682/682 [==============================] - 101s 149ms/step - loss: 0.2290 - accuracy: 0.9024 - val_loss: 0.2541 - val_accuracy: 0.8940\n",
      "Epoch 39/50\n",
      "682/682 [==============================] - 97s 142ms/step - loss: 0.2230 - accuracy: 0.9050 - val_loss: 0.2583 - val_accuracy: 0.8926\n",
      "Epoch 40/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.2195 - accuracy: 0.9063 - val_loss: 0.2418 - val_accuracy: 0.8986\n",
      "Epoch 41/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.2153 - accuracy: 0.9081 - val_loss: 0.2348 - val_accuracy: 0.9006\n",
      "Epoch 42/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.2097 - accuracy: 0.9106 - val_loss: 0.2344 - val_accuracy: 0.9006\n",
      "Epoch 43/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.2063 - accuracy: 0.9115 - val_loss: 0.2392 - val_accuracy: 0.8977\n",
      "Epoch 44/50\n",
      "682/682 [==============================] - 92s 134ms/step - loss: 0.2041 - accuracy: 0.9126 - val_loss: 0.2244 - val_accuracy: 0.9047\n",
      "Epoch 45/50\n",
      "682/682 [==============================] - 91s 134ms/step - loss: 0.1983 - accuracy: 0.9150 - val_loss: 0.2888 - val_accuracy: 0.8832\n",
      "Epoch 46/50\n",
      "682/682 [==============================] - 91s 134ms/step - loss: 0.1961 - accuracy: 0.9159 - val_loss: 0.2038 - val_accuracy: 0.9121\n",
      "Epoch 47/50\n",
      "682/682 [==============================] - 92s 134ms/step - loss: 0.1928 - accuracy: 0.9174 - val_loss: 0.1991 - val_accuracy: 0.9170\n",
      "Epoch 48/50\n",
      "682/682 [==============================] - 91s 134ms/step - loss: 0.1875 - accuracy: 0.9197 - val_loss: 0.1928 - val_accuracy: 0.9184\n",
      "Epoch 49/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.1854 - accuracy: 0.9204 - val_loss: 0.1946 - val_accuracy: 0.9169\n",
      "Epoch 50/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1816 - accuracy: 0.9218 - val_loss: 0.1995 - val_accuracy: 0.9152\n",
      "(None, 24, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla, lstm_cell_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/cnn_lstm_7days_history_1days_pred_50_epochs_without_lambda\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/cnn_lstm_7days_history_1days_pred_50_epochs_without_lambda\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 1)\n",
      "(None, 24, 1)\n",
      "Epoch 1/50\n",
      "682/682 [==============================] - 88s 129ms/step - loss: 0.1772 - accuracy: 0.9243 - val_loss: 0.2028 - val_accuracy: 0.9136\n",
      "Epoch 2/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.1755 - accuracy: 0.9247 - val_loss: 0.1889 - val_accuracy: 0.9194\n",
      "Epoch 3/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.1729 - accuracy: 0.9258 - val_loss: 0.1837 - val_accuracy: 0.9214\n",
      "Epoch 4/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1721 - accuracy: 0.9257 - val_loss: 0.1871 - val_accuracy: 0.9194\n",
      "Epoch 5/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.1695 - accuracy: 0.9269 - val_loss: 0.1814 - val_accuracy: 0.9236\n",
      "Epoch 6/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1672 - accuracy: 0.9280 - val_loss: 0.1892 - val_accuracy: 0.9196\n",
      "Epoch 7/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1633 - accuracy: 0.9297 - val_loss: 0.1907 - val_accuracy: 0.9181\n",
      "Epoch 8/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1618 - accuracy: 0.9304 - val_loss: 0.1704 - val_accuracy: 0.9264\n",
      "Epoch 9/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1608 - accuracy: 0.9308 - val_loss: 0.1784 - val_accuracy: 0.9241\n",
      "Epoch 10/50\n",
      "682/682 [==============================] - 90s 133ms/step - loss: 0.1591 - accuracy: 0.9315 - val_loss: 0.1699 - val_accuracy: 0.9266\n",
      "Epoch 11/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1565 - accuracy: 0.9327 - val_loss: 0.1700 - val_accuracy: 0.9258\n",
      "Epoch 12/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1555 - accuracy: 0.9331 - val_loss: 0.1667 - val_accuracy: 0.9276\n",
      "Epoch 13/50\n",
      "682/682 [==============================] - 90s 133ms/step - loss: 0.1544 - accuracy: 0.9335 - val_loss: 0.1661 - val_accuracy: 0.9280\n",
      "Epoch 14/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1511 - accuracy: 0.9348 - val_loss: 0.1648 - val_accuracy: 0.9291\n",
      "Epoch 15/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1499 - accuracy: 0.9352 - val_loss: 0.1566 - val_accuracy: 0.9337\n",
      "Epoch 16/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1488 - accuracy: 0.9359 - val_loss: 0.1750 - val_accuracy: 0.9242\n",
      "Epoch 17/50\n",
      "682/682 [==============================] - 90s 133ms/step - loss: 0.1462 - accuracy: 0.9365 - val_loss: 0.1549 - val_accuracy: 0.9333\n",
      "Epoch 18/50\n",
      "682/682 [==============================] - 91s 133ms/step - loss: 0.1456 - accuracy: 0.9371 - val_loss: 0.1629 - val_accuracy: 0.9300\n",
      "Epoch 19/50\n",
      "682/682 [==============================] - 90s 132ms/step - loss: 0.1433 - accuracy: 0.9379 - val_loss: 0.1573 - val_accuracy: 0.9326\n",
      "Epoch 20/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1426 - accuracy: 0.9385 - val_loss: 0.1656 - val_accuracy: 0.9290\n",
      "Epoch 21/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.1412 - accuracy: 0.9390 - val_loss: 0.1537 - val_accuracy: 0.9340\n",
      "Epoch 22/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1396 - accuracy: 0.9395 - val_loss: 0.1617 - val_accuracy: 0.9306\n",
      "Epoch 23/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1380 - accuracy: 0.9402 - val_loss: 0.1569 - val_accuracy: 0.9324\n",
      "Epoch 24/50\n",
      "682/682 [==============================] - 86s 126ms/step - loss: 0.1360 - accuracy: 0.9412 - val_loss: 0.1543 - val_accuracy: 0.9329\n",
      "Epoch 25/50\n",
      "682/682 [==============================] - 86s 126ms/step - loss: 0.1347 - accuracy: 0.9416 - val_loss: 0.1526 - val_accuracy: 0.9331\n",
      "Epoch 26/50\n",
      "682/682 [==============================] - 86s 127ms/step - loss: 0.1342 - accuracy: 0.9417 - val_loss: 0.1636 - val_accuracy: 0.9296\n",
      "Epoch 27/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1324 - accuracy: 0.9423 - val_loss: 0.1523 - val_accuracy: 0.9339\n",
      "Epoch 28/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1320 - accuracy: 0.9427 - val_loss: 0.1516 - val_accuracy: 0.9337\n",
      "Epoch 29/50\n",
      "682/682 [==============================] - 86s 126ms/step - loss: 0.1295 - accuracy: 0.9440 - val_loss: 0.1542 - val_accuracy: 0.9342\n",
      "Epoch 30/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1284 - accuracy: 0.9444 - val_loss: 0.1461 - val_accuracy: 0.9370\n",
      "Epoch 31/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.1279 - accuracy: 0.9448 - val_loss: 0.1680 - val_accuracy: 0.9287\n",
      "Epoch 32/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.1268 - accuracy: 0.9455 - val_loss: 0.1397 - val_accuracy: 0.9390\n",
      "Epoch 33/50\n",
      "682/682 [==============================] - 88s 128ms/step - loss: 0.1263 - accuracy: 0.9451 - val_loss: 0.1401 - val_accuracy: 0.9393\n",
      "Epoch 34/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.1242 - accuracy: 0.9461 - val_loss: 0.1353 - val_accuracy: 0.9418\n",
      "Epoch 35/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.1227 - accuracy: 0.9470 - val_loss: 0.1487 - val_accuracy: 0.9357\n",
      "Epoch 36/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.1224 - accuracy: 0.9472 - val_loss: 0.1411 - val_accuracy: 0.9386\n",
      "Epoch 37/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.1215 - accuracy: 0.9477 - val_loss: 0.1365 - val_accuracy: 0.9401\n",
      "Epoch 38/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1201 - accuracy: 0.9483 - val_loss: 0.1384 - val_accuracy: 0.9396\n",
      "Epoch 39/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1192 - accuracy: 0.9486 - val_loss: 0.1443 - val_accuracy: 0.9385\n",
      "Epoch 40/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1176 - accuracy: 0.9491 - val_loss: 0.1371 - val_accuracy: 0.9403\n",
      "Epoch 41/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1175 - accuracy: 0.9493 - val_loss: 0.1304 - val_accuracy: 0.9442\n",
      "Epoch 42/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1166 - accuracy: 0.9496 - val_loss: 0.1427 - val_accuracy: 0.9391\n",
      "Epoch 43/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1159 - accuracy: 0.9501 - val_loss: 0.1382 - val_accuracy: 0.9410\n",
      "Epoch 44/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.1146 - accuracy: 0.9505 - val_loss: 0.1265 - val_accuracy: 0.9445\n",
      "Epoch 45/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1139 - accuracy: 0.9509 - val_loss: 0.1290 - val_accuracy: 0.9444\n",
      "Epoch 46/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1126 - accuracy: 0.9516 - val_loss: 0.1225 - val_accuracy: 0.9466\n",
      "Epoch 47/50\n",
      "682/682 [==============================] - 87s 128ms/step - loss: 0.1119 - accuracy: 0.9514 - val_loss: 0.1270 - val_accuracy: 0.9456\n",
      "Epoch 48/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1110 - accuracy: 0.9521 - val_loss: 0.1332 - val_accuracy: 0.9434\n",
      "Epoch 49/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1099 - accuracy: 0.9529 - val_loss: 0.1346 - val_accuracy: 0.9422\n",
      "Epoch 50/50\n",
      "682/682 [==============================] - 87s 127ms/step - loss: 0.1095 - accuracy: 0.9530 - val_loss: 0.1319 - val_accuracy: 0.9440\n",
      "(None, 24, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla, lstm_cell_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/cnn_lstm_7days_history_1days_pred_100_epochs_without_lambda\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/cnn_lstm_7days_history_1days_pred_100_epochs_without_lambda\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 1)\n",
      "(None, 24, 1)\n",
      "Epoch 1/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.1080 - accuracy: 0.9538 - val_loss: 0.1304 - val_accuracy: 0.9442\n",
      "Epoch 2/50\n",
      "682/682 [==============================] - 88s 130ms/step - loss: 0.1083 - accuracy: 0.9539 - val_loss: 0.1363 - val_accuracy: 0.9421\n",
      "Epoch 3/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.1060 - accuracy: 0.9546 - val_loss: 0.1316 - val_accuracy: 0.9436\n",
      "Epoch 4/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.1060 - accuracy: 0.9547 - val_loss: 0.1573 - val_accuracy: 0.9341\n",
      "Epoch 5/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.1054 - accuracy: 0.9551 - val_loss: 0.1264 - val_accuracy: 0.9450\n",
      "Epoch 6/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.1044 - accuracy: 0.9554 - val_loss: 0.1168 - val_accuracy: 0.9496\n",
      "Epoch 7/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.1032 - accuracy: 0.9561 - val_loss: 0.1299 - val_accuracy: 0.9438\n",
      "Epoch 8/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.1035 - accuracy: 0.9559 - val_loss: 0.1234 - val_accuracy: 0.9464\n",
      "Epoch 9/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.1021 - accuracy: 0.9566 - val_loss: 0.1257 - val_accuracy: 0.9455\n",
      "Epoch 10/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.1017 - accuracy: 0.9570 - val_loss: 0.1203 - val_accuracy: 0.9478\n",
      "Epoch 11/50\n",
      "682/682 [==============================] - 88s 130ms/step - loss: 0.1005 - accuracy: 0.9571 - val_loss: 0.1272 - val_accuracy: 0.9450\n",
      "Epoch 12/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0996 - accuracy: 0.9578 - val_loss: 0.1185 - val_accuracy: 0.9494\n",
      "Epoch 13/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.0990 - accuracy: 0.9581 - val_loss: 0.1155 - val_accuracy: 0.9504\n",
      "Epoch 14/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0986 - accuracy: 0.9582 - val_loss: 0.1158 - val_accuracy: 0.9508\n",
      "Epoch 15/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.0977 - accuracy: 0.9589 - val_loss: 0.1145 - val_accuracy: 0.9508\n",
      "Epoch 16/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0973 - accuracy: 0.9585 - val_loss: 0.1278 - val_accuracy: 0.9449\n",
      "Epoch 17/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0975 - accuracy: 0.9585 - val_loss: 0.1112 - val_accuracy: 0.9521\n",
      "Epoch 18/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.0959 - accuracy: 0.9593 - val_loss: 0.1154 - val_accuracy: 0.9511\n",
      "Epoch 19/50\n",
      "682/682 [==============================] - 88s 130ms/step - loss: 0.0951 - accuracy: 0.9598 - val_loss: 0.1223 - val_accuracy: 0.9475\n",
      "Epoch 20/50\n",
      "682/682 [==============================] - 88s 130ms/step - loss: 0.0959 - accuracy: 0.9595 - val_loss: 0.1142 - val_accuracy: 0.9504\n",
      "Epoch 21/50\n",
      "682/682 [==============================] - 88s 130ms/step - loss: 0.0946 - accuracy: 0.9601 - val_loss: 0.1163 - val_accuracy: 0.9496\n",
      "Epoch 22/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0934 - accuracy: 0.9607 - val_loss: 0.1072 - val_accuracy: 0.9538\n",
      "Epoch 23/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0933 - accuracy: 0.9606 - val_loss: 0.1157 - val_accuracy: 0.9507\n",
      "Epoch 24/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0921 - accuracy: 0.9614 - val_loss: 0.1094 - val_accuracy: 0.9532\n",
      "Epoch 25/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0921 - accuracy: 0.9612 - val_loss: 0.1099 - val_accuracy: 0.9529\n",
      "Epoch 26/50\n",
      "682/682 [==============================] - 88s 129ms/step - loss: 0.0908 - accuracy: 0.9620 - val_loss: 0.1041 - val_accuracy: 0.9553\n",
      "Epoch 27/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0914 - accuracy: 0.9615 - val_loss: 0.1322 - val_accuracy: 0.9443\n",
      "Epoch 28/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0913 - accuracy: 0.9616 - val_loss: 0.1099 - val_accuracy: 0.9533\n",
      "Epoch 29/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0896 - accuracy: 0.9623 - val_loss: 0.1100 - val_accuracy: 0.9525\n",
      "Epoch 30/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0898 - accuracy: 0.9626 - val_loss: 0.1082 - val_accuracy: 0.9539\n",
      "Epoch 31/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.0903 - accuracy: 0.9621 - val_loss: 0.1107 - val_accuracy: 0.9534\n",
      "Epoch 32/50\n",
      "682/682 [==============================] - 88s 129ms/step - loss: 0.0888 - accuracy: 0.9626 - val_loss: 0.1104 - val_accuracy: 0.9539\n",
      "Epoch 33/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0877 - accuracy: 0.9632 - val_loss: 0.1031 - val_accuracy: 0.9563\n",
      "Epoch 34/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.0883 - accuracy: 0.9630 - val_loss: 0.1020 - val_accuracy: 0.9571\n",
      "Epoch 35/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0871 - accuracy: 0.9639 - val_loss: 0.1085 - val_accuracy: 0.9541\n",
      "Epoch 36/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0863 - accuracy: 0.9641 - val_loss: 0.1122 - val_accuracy: 0.9522\n",
      "Epoch 37/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0862 - accuracy: 0.9642 - val_loss: 0.1082 - val_accuracy: 0.9541\n",
      "Epoch 38/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0863 - accuracy: 0.9638 - val_loss: 0.1127 - val_accuracy: 0.9525\n",
      "Epoch 39/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0855 - accuracy: 0.9644 - val_loss: 0.1073 - val_accuracy: 0.9547\n",
      "Epoch 40/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0850 - accuracy: 0.9646 - val_loss: 0.1605 - val_accuracy: 0.9364\n",
      "Epoch 41/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0850 - accuracy: 0.9647 - val_loss: 0.0985 - val_accuracy: 0.9576\n",
      "Epoch 42/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0840 - accuracy: 0.9654 - val_loss: 0.1026 - val_accuracy: 0.9570\n",
      "Epoch 43/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.0851 - accuracy: 0.9646 - val_loss: 0.1038 - val_accuracy: 0.9558\n",
      "Epoch 44/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0836 - accuracy: 0.9655 - val_loss: 0.0977 - val_accuracy: 0.9588\n",
      "Epoch 45/50\n",
      "682/682 [==============================] - 89s 131ms/step - loss: 0.0834 - accuracy: 0.9654 - val_loss: 0.0997 - val_accuracy: 0.9578\n",
      "Epoch 46/50\n",
      "682/682 [==============================] - 90s 131ms/step - loss: 0.0842 - accuracy: 0.9651 - val_loss: 0.1074 - val_accuracy: 0.9546\n",
      "Epoch 47/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0829 - accuracy: 0.9656 - val_loss: 0.0979 - val_accuracy: 0.9586\n",
      "Epoch 48/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0817 - accuracy: 0.9663 - val_loss: 0.1000 - val_accuracy: 0.9573\n",
      "Epoch 49/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0814 - accuracy: 0.9666 - val_loss: 0.1028 - val_accuracy: 0.9564\n",
      "Epoch 50/50\n",
      "682/682 [==============================] - 89s 130ms/step - loss: 0.0813 - accuracy: 0.9664 - val_loss: 0.1021 - val_accuracy: 0.9572\n",
      "(None, 24, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla, lstm_cell_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/cnn_lstm_7days_history_1days_pred_150_epochs_without_lambda\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/cnn_lstm_7days_history_1days_pred_150_epochs_without_lambda\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 1)\n",
      "(None, 24, 1)\n",
      "Epoch 1/50\n",
      "682/682 [==============================] - 92s 134ms/step - loss: 0.0809 - accuracy: 0.9666 - val_loss: 0.1067 - val_accuracy: 0.9543\n",
      "Epoch 2/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0807 - accuracy: 0.9670 - val_loss: 0.1038 - val_accuracy: 0.9570\n",
      "Epoch 3/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0802 - accuracy: 0.9670 - val_loss: 0.1053 - val_accuracy: 0.9556\n",
      "Epoch 4/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0799 - accuracy: 0.9673 - val_loss: 0.1024 - val_accuracy: 0.9574\n",
      "Epoch 5/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0787 - accuracy: 0.9679 - val_loss: 0.1003 - val_accuracy: 0.9582\n",
      "Epoch 6/50\n",
      "682/682 [==============================] - 92s 136ms/step - loss: 0.0796 - accuracy: 0.9670 - val_loss: 0.0976 - val_accuracy: 0.9586\n",
      "Epoch 7/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0789 - accuracy: 0.9676 - val_loss: 0.0958 - val_accuracy: 0.9589\n",
      "Epoch 8/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0793 - accuracy: 0.9673 - val_loss: 0.0987 - val_accuracy: 0.9583\n",
      "Epoch 9/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0791 - accuracy: 0.9673 - val_loss: 0.0994 - val_accuracy: 0.9575\n",
      "Epoch 10/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0786 - accuracy: 0.9676 - val_loss: 0.1059 - val_accuracy: 0.9555\n",
      "Epoch 11/50\n",
      "682/682 [==============================] - 92s 136ms/step - loss: 0.0782 - accuracy: 0.9679 - val_loss: 0.0984 - val_accuracy: 0.9588\n",
      "Epoch 12/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0777 - accuracy: 0.9681 - val_loss: 0.0990 - val_accuracy: 0.9580\n",
      "Epoch 13/50\n",
      "682/682 [==============================] - 92s 136ms/step - loss: 0.0770 - accuracy: 0.9682 - val_loss: 0.1070 - val_accuracy: 0.9555\n",
      "Epoch 14/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0770 - accuracy: 0.9684 - val_loss: 0.0982 - val_accuracy: 0.9595\n",
      "Epoch 15/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0761 - accuracy: 0.9690 - val_loss: 0.0966 - val_accuracy: 0.9593\n",
      "Epoch 16/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0762 - accuracy: 0.9687 - val_loss: 0.1006 - val_accuracy: 0.9583\n",
      "Epoch 17/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0758 - accuracy: 0.9689 - val_loss: 0.0934 - val_accuracy: 0.9599\n",
      "Epoch 18/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0763 - accuracy: 0.9686 - val_loss: 0.0956 - val_accuracy: 0.9602\n",
      "Epoch 19/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0757 - accuracy: 0.9690 - val_loss: 0.0974 - val_accuracy: 0.9588\n",
      "Epoch 20/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0746 - accuracy: 0.9693 - val_loss: 0.1000 - val_accuracy: 0.9580\n",
      "Epoch 21/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0745 - accuracy: 0.9696 - val_loss: 0.1080 - val_accuracy: 0.9546\n",
      "Epoch 22/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0745 - accuracy: 0.9696 - val_loss: 0.0967 - val_accuracy: 0.9594\n",
      "Epoch 23/50\n",
      "682/682 [==============================] - 92s 136ms/step - loss: 0.0743 - accuracy: 0.9696 - val_loss: 0.0998 - val_accuracy: 0.9585\n",
      "Epoch 24/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0745 - accuracy: 0.9697 - val_loss: 0.0947 - val_accuracy: 0.9597\n",
      "Epoch 25/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0742 - accuracy: 0.9696 - val_loss: 0.0971 - val_accuracy: 0.9595\n",
      "Epoch 26/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0737 - accuracy: 0.9701 - val_loss: 0.0969 - val_accuracy: 0.9596\n",
      "Epoch 27/50\n",
      "682/682 [==============================] - 92s 134ms/step - loss: 0.0735 - accuracy: 0.9701 - val_loss: 0.1015 - val_accuracy: 0.9584\n",
      "Epoch 28/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0725 - accuracy: 0.9707 - val_loss: 0.0911 - val_accuracy: 0.9619\n",
      "Epoch 29/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0733 - accuracy: 0.9702 - val_loss: 0.0991 - val_accuracy: 0.9590\n",
      "Epoch 30/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0724 - accuracy: 0.9706 - val_loss: 0.0928 - val_accuracy: 0.9611\n",
      "Epoch 31/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0724 - accuracy: 0.9706 - val_loss: 0.0924 - val_accuracy: 0.9611\n",
      "Epoch 32/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0720 - accuracy: 0.9709 - val_loss: 0.0917 - val_accuracy: 0.9618\n",
      "Epoch 33/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0719 - accuracy: 0.9709 - val_loss: 0.0929 - val_accuracy: 0.9608\n",
      "Epoch 34/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0715 - accuracy: 0.9710 - val_loss: 0.0937 - val_accuracy: 0.9613\n",
      "Epoch 35/50\n",
      "682/682 [==============================] - 92s 136ms/step - loss: 0.0714 - accuracy: 0.9710 - val_loss: 0.0920 - val_accuracy: 0.9613\n",
      "Epoch 36/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0711 - accuracy: 0.9711 - val_loss: 0.0869 - val_accuracy: 0.9639\n",
      "Epoch 37/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0709 - accuracy: 0.9712 - val_loss: 0.0912 - val_accuracy: 0.9619\n",
      "Epoch 38/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0710 - accuracy: 0.9711 - val_loss: 0.0952 - val_accuracy: 0.9611\n",
      "Epoch 39/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0704 - accuracy: 0.9714 - val_loss: 0.0887 - val_accuracy: 0.9631\n",
      "Epoch 40/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0706 - accuracy: 0.9715 - val_loss: 0.0903 - val_accuracy: 0.9630\n",
      "Epoch 41/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0698 - accuracy: 0.9717 - val_loss: 0.0896 - val_accuracy: 0.9619\n",
      "Epoch 42/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0697 - accuracy: 0.9719 - val_loss: 0.0907 - val_accuracy: 0.9622\n",
      "Epoch 43/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0693 - accuracy: 0.9720 - val_loss: 0.0922 - val_accuracy: 0.9616\n",
      "Epoch 44/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0696 - accuracy: 0.9718 - val_loss: 0.0970 - val_accuracy: 0.9601\n",
      "Epoch 45/50\n",
      "682/682 [==============================] - 93s 136ms/step - loss: 0.0696 - accuracy: 0.9718 - val_loss: 0.0926 - val_accuracy: 0.9615\n",
      "Epoch 46/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0686 - accuracy: 0.9722 - val_loss: 0.0895 - val_accuracy: 0.9629\n",
      "Epoch 47/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0688 - accuracy: 0.9722 - val_loss: 0.1009 - val_accuracy: 0.9580\n",
      "Epoch 48/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0682 - accuracy: 0.9724 - val_loss: 0.0909 - val_accuracy: 0.9619\n",
      "Epoch 49/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0683 - accuracy: 0.9727 - val_loss: 0.0897 - val_accuracy: 0.9626\n",
      "Epoch 50/50\n",
      "682/682 [==============================] - 92s 135ms/step - loss: 0.0680 - accuracy: 0.9725 - val_loss: 0.0910 - val_accuracy: 0.9621\n",
      "(None, 24, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla, lstm_cell_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/cnn_lstm_7days_history_1days_pred_200_epochs_without_lambda\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/cnn_lstm_7days_history_1days_pred_200_epochs_without_lambda\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 1 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/data-mining-team2/model-tests/e/MOD-6/metadata\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_model(False)\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\"accuracy\"])\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"data-mining-team2/model-tests\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4Mzg2ZWZmYi05YzRlLTQ3ODYtOWE1NC1mNDM4OTM1ZjNlOTkifQ==\",\n",
    "    custom_run_id=\"classification_without_lambda\",\n",
    "    source_files=[\"./model_tests.ipynb\"],\n",
    ")\n",
    "\n",
    "run[\"parameters\"] = {\n",
    "    \"include_lambda\": False\n",
    "}\n",
    "\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace=\"training\")\n",
    "\n",
    "\n",
    "for epoch_count_factor in range(1, 5):\n",
    "    epoch_count = 50 * epoch_count_factor\n",
    "    history = model.fit(\n",
    "        window_generator_538.train,\n",
    "        epochs=50,\n",
    "        validation_data=window_generator_538.val,\n",
    "        callbacks=[neptune_cbk],\n",
    "    )\n",
    "\n",
    "    eval_metrics = model.evaluate(window_generator_538.test, verbose=0)\n",
    "    for j, metric in enumerate(eval_metrics):\n",
    "        run[f\"eval/epoch_{epoch_count}/{model.metrics_names[j]}\"] = metric\n",
    "\n",
    "    name = f\"cnn_lstm_7days_history_1days_pred_{epoch_count}_epochs_without_lambda\"\n",
    "\n",
    "    model.save(\"../models/\" + name)\n",
    "\n",
    "    run[\"model_names/\" + str(epoch_count) + \"epochs\"] = name\n",
    "    \n",
    "run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/data-mining-team2/model-tests/e/MOD-7\n",
      "(None, 24, 1)\n",
      "(None, 24, 1)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\backend.py\", line 6523, in pool2d\n        x = tf.compat.v1.nn.max_pool(\n\n    ValueError: Exception encountered when calling layer 'max_pooling1d_2' (type MaxPooling1D).\n    \n    Negative dimension size caused by subtracting 2 from 1 for '{{node sequential_1/max_pooling1d_2/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 1, 1], padding=\"VALID\", strides=[1, 2, 1, 1]](sequential_1/max_pooling1d_2/ExpandDims)' with input shapes: [?,1,1,256].\n    \n    Call arguments received by layer 'max_pooling1d_2' (type MaxPooling1D):\n      â¢ inputs=tf.Tensor(shape=(None, 1, 256), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m epoch_count_factor \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m     21\u001b[0m     epoch_count \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m \u001b[39m*\u001b[39m epoch_count_factor\n\u001b[1;32m---> 22\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     23\u001b[0m         window_generator_538\u001b[39m.\u001b[39;49mtrain,\n\u001b[0;32m     24\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m     25\u001b[0m         validation_data\u001b[39m=\u001b[39;49mwindow_generator_538\u001b[39m.\u001b[39;49mval,\n\u001b[0;32m     26\u001b[0m         callbacks\u001b[39m=\u001b[39;49m[neptune_cbk],\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     29\u001b[0m     eval_metrics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(window_generator_538\u001b[39m.\u001b[39mtest, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \u001b[39mfor\u001b[39;00m j, metric \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(eval_metrics):\n",
      "File \u001b[1;32me:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file0ugk3w9t.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\backend.py\", line 6523, in pool2d\n        x = tf.compat.v1.nn.max_pool(\n\n    ValueError: Exception encountered when calling layer 'max_pooling1d_2' (type MaxPooling1D).\n    \n    Negative dimension size caused by subtracting 2 from 1 for '{{node sequential_1/max_pooling1d_2/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 1, 1], padding=\"VALID\", strides=[1, 2, 1, 1]](sequential_1/max_pooling1d_2/ExpandDims)' with input shapes: [?,1,1,256].\n    \n    Call arguments received by layer 'max_pooling1d_2' (type MaxPooling1D):\n      â¢ inputs=tf.Tensor(shape=(None, 1, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(True)\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=[\"accuracy\"])\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"data-mining-team2/model-tests\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4Mzg2ZWZmYi05YzRlLTQ3ODYtOWE1NC1mNDM4OTM1ZjNlOTkifQ==\",\n",
    "    custom_run_id=\"classification_with_lambda\",\n",
    "    source_files=[\"./model_tests.ipynb\"],\n",
    ")\n",
    "\n",
    "run[\"parameters\"] = {\n",
    "    \"include_lambda\": True\n",
    "}\n",
    "\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace=\"training\")\n",
    "\n",
    "\n",
    "for epoch_count_factor in range(1, 5):\n",
    "    epoch_count = 50 * epoch_count_factor\n",
    "    history = model.fit(\n",
    "        window_generator_538.train,\n",
    "        epochs=50,\n",
    "        validation_data=window_generator_538.val,\n",
    "        callbacks=[neptune_cbk],\n",
    "    )\n",
    "\n",
    "    eval_metrics = model.evaluate(window_generator_538.test, verbose=0)\n",
    "    for j, metric in enumerate(eval_metrics):\n",
    "        run[f\"eval/epoch_{epoch_count}/{model.metrics_names[j]}\"] = metric\n",
    "\n",
    "    name = f\"cnn_lstm_7days_history_1days_pred_{epoch_count}_epochs_with_lambda\"\n",
    "\n",
    "    model.save(\"../models/\" + name)\n",
    "\n",
    "    run[\"model_names/\" + str(epoch_count) + \"epochs\"] = name\n",
    "    \n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model = tf.keras.models.load_model(\"../models/cnn_lstm_7days_history_1days_pred_100_epochs_without_lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 168, 11), found shape=(None, 336, 11)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred \u001b[39m=\u001b[39m cnn_lstm_model\u001b[39m.\u001b[39;49mpredict(window_generator_538\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[1;32me:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_56e5nux.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"e:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 168, 11), found shape=(None, 336, 11)\n"
     ]
    }
   ],
   "source": [
    "pred = cnn_lstm_model.predict(window_generator_538.train.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 24, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 168, 11), found shape=(3, 336, 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m window_generator_538\u001b[39m.\u001b[39;49mplot(cnn_lstm_model, plot_col\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpm10\u001b[39;49m\u001b[39m\"\u001b[39;49m, offset\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, plot_version\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32me:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\src\\model\\window_generator_categorical.py:125\u001b[0m, in \u001b[0;36mWindowGenerator.plot\u001b[1;34m(self, model, plot_col, max_subplots, offset, plot_version)\u001b[0m\n\u001b[0;32m    113\u001b[0m     fig\u001b[39m.\u001b[39madd_trace(\n\u001b[0;32m    114\u001b[0m         go\u001b[39m.\u001b[39mScatter(\n\u001b[0;32m    115\u001b[0m             x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_indices,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m         col\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m         predictions \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m    126\u001b[0m         fig\u001b[39m.\u001b[39madd_trace(\n\u001b[0;32m    127\u001b[0m             go\u001b[39m.\u001b[39mScatter(\n\u001b[0;32m    128\u001b[0m                 x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_indices,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m             col\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m    136\u001b[0m         )\n\u001b[0;32m    138\u001b[0m fig\u001b[39m.\u001b[39mupdate_yaxes(title_text\u001b[39m=\u001b[39mplot_col, row\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, col\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32me:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\FHBielefeld\\Master\\DataMining\\feinstaubprojekt-polen\\.env\\lib\\site-packages\\keras\\engine\\input_spec.py:298\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m spec_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m     \u001b[39mif\u001b[39;00m spec_dim \u001b[39m!=\u001b[39m dim:\n\u001b[1;32m--> 298\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    300\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected shape=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound shape=\u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 168, 11), found shape=(3, 336, 11)"
     ]
    }
   ],
   "source": [
    "window_generator_538.plot(cnn_lstm_model, plot_col=\"kategorie\", offset=500, plot_version=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
